{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a7c66c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20188/1322910605.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Are there any useless columns?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# The three obvious outliers seem to be gone, the rest of the data seems to be under a more linear relationship between the variables\n",
    "\n",
    "# Are there any useless columns? \n",
    "\n",
    "df.info()\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Can drop PID as its just an ID but we already have index as ID\n",
    "\n",
    "df = df.drop('PID',axis=1)\n",
    "\n",
    "len(df.columns)\n",
    "\n",
    "# Let's find missing values\n",
    "\n",
    "df.isnull().sum().sort_values()\n",
    "\n",
    "(100*df.isnull().sum()/len(df)).sort_values()\n",
    "\n",
    "# Need a function that is going to calculate percentage of null values for me so I can check \n",
    "# if altering the df changes my missing values\n",
    "\n",
    "def percent_missing(df):\n",
    "    percent_nan = 100* df.isnull().sum() / len(df)\n",
    "    percent_nan = percent_nan[percent_nan>0].sort_values()\n",
    "    return percent_nan\n",
    "\n",
    "percent_nan = percent_missing(df)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(y=percent_nan.index,x=percent_nan)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(x=percent_nan,y=percent_nan.index)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "percent_nan[percent_nan <1]\n",
    "\n",
    "# how many rows are these percentages?\n",
    "\n",
    "100/len(df)\n",
    "\n",
    "# Is the row missing electrical the same as garage area for example?\n",
    "\n",
    "df[df['Electrical'].isnull()]['Garage Area']\n",
    "\n",
    "# Let's see if dropping the row that is missing electrical and garage cars will also reduce my missing value rates\n",
    "\n",
    "df=df.dropna(axis = 0, subset = ['Electrical','Garage Cars'])\n",
    "\n",
    "percent_nan = percent_missing(df)\n",
    "\n",
    "percent_nan[percent_nan <1]\n",
    "\n",
    "# Since there are so many columns with missing values where basement is mentioned maybe should look if they \n",
    "# are for the same houses.\n",
    "\n",
    "# What if they just don't have a basement?\n",
    "\n",
    "df[df['Bsmt Unf SF'].isnull()][['Total Bsmt SF','BsmtFin SF 1','BsmtFin SF 2','Bsmt Full Bath','Bsmt Half Bath']]\n",
    "\n",
    "# This makes it much easier for loc 1341\n",
    "\n",
    "# Can fill it with basement = 'None' since it's probably just missing a basement\n",
    "\n",
    "bsmt_num_cols = ['BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF','Total Bsmt SF', 'Bsmt Full Bath', 'Bsmt Half Bath']\n",
    "df[bsmt_num_cols] = df[bsmt_num_cols].fillna(0)\n",
    "\n",
    "bsmt_str_cols =  ['Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin Type 2']\n",
    "df[bsmt_str_cols] = df[bsmt_str_cols].fillna('None')\n",
    "\n",
    "percent_nan = percent_missing(df)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(x=percent_nan,y=percent_nan.index)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "# Looks like Mas Vnr is for houses that are missing masonry vaneer so let's just fill those like we did with basement\n",
    "\n",
    "df[\"Mas Vnr Type\"] = df[\"Mas Vnr Type\"].fillna(\"None\")\n",
    "df[\"Mas Vnr Area\"] = df[\"Mas Vnr Area\"].fillna(0)\n",
    "\n",
    "percent_nan = percent_missing(df)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(x=percent_nan,y=percent_nan.index)\n",
    "\n",
    "# Since there are no more features that are under the 1% missing data threshold I won't drop any more rows\n",
    "# Garage columns all seem to be missing the same amount of data and in the description it says if they are missing data\n",
    "# then the house just doesn't have a garage so let's normalize this\n",
    "\n",
    "gar_str_cols = ['Garage Type', 'Garage Finish', 'Garage Qual', 'Garage Cond']\n",
    "df[gar_str_cols] = df[gar_str_cols].fillna('None')\n",
    "\n",
    "df['Garage Yr Blt'] = df['Garage Yr Blt'].fillna(0)\n",
    "\n",
    "percent_nan=percent_missing(df)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(x=percent_nan,y=percent_nan.index)\n",
    "\n",
    "# Will be dropping columns that are missing such a high percentage of rows like The bottom 4 here ^^\n",
    "\n",
    "df = df.drop(['Fence','Alley','Misc Feature','Pool QC'],axis = 1)\n",
    "\n",
    "# And let's inspect the other features\n",
    "\n",
    "df['Fireplace Qu'].value_counts()\n",
    "\n",
    "# Since these are just string values I will fill NA's with None again\n",
    "\n",
    "df['Fireplace Qu'] = df['Fireplace Qu'].fillna('None')\n",
    "\n",
    "percent_nan=percent_missing(df)\n",
    "percent_nan\n",
    "\n",
    "# All that is missing is Lot Frontage\n",
    "# In this case what I can do is look at the average of this feature \n",
    "# in  the feature 'Neighborhoods' and fill missing values with the mean for that neighborhood\n",
    "# This method assumes that neighborhood have pretty unifrom Lot Frontages for all the houses there\n",
    "# which I am okay with since that is how it usually\n",
    "\n",
    "df[df['Lot Frontage'].isnull()]\n",
    "\n",
    "df.groupby('Neighborhood')['Lot Frontage'].mean()\n",
    "\n",
    "df['Lot Frontage'] = df.groupby('Neighborhood')['Lot Frontage'].transform(lambda val: val.fillna(val.mean()))\n",
    "\n",
    "df[df['Lot Frontage'].isnull()]['Neighborhood']\n",
    "\n",
    "# Since GrnHill and Landmrk don't have means of their Lot Frontages I will assume they don't have them\n",
    "\n",
    "df['Lot Frontage'] = df['Lot Frontage'].fillna(0)\n",
    "\n",
    "percent_nan=percent_missing(df)\n",
    "percent_nan\n",
    "\n",
    "df.info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Yay no more missing values!\n",
    "# Now let's look at other values that may need to be split out through get_dummies or transformed\n",
    "\n",
    "df['MS SubClass'] = df['MS SubClass'].apply(str)\n",
    "\n",
    "nums = df.select_dtypes(exclude='object')\n",
    "objs = df.select_dtypes(include='object')\n",
    "\n",
    "objs = pd.get_dummies(objs,drop_first = True)\n",
    "\n",
    "df= pd.concat([nums,objs],axis=1)\n",
    "\n",
    "df.info()\n",
    "\n",
    "# The df is finalized and ready to be worked on\n",
    "\n",
    "X = df.drop('SalePrice',axis = 1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=69)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "base_elastic_model = ElasticNet()\n",
    "\n",
    "params = {'alpha':[0.1,1,5,10,50,100],\n",
    "              'l1_ratio':[.1, .5, .7, .9, .95, .99, 1]}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_model = GridSearchCV(estimator = base_elastic_model, param_grid=params, cv =5, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_model.fit(X_train,y_train)\n",
    "\n",
    "grid_model.best_params_\n",
    "\n",
    "y_pred = grid_model.predict(scaled_X_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a13c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
